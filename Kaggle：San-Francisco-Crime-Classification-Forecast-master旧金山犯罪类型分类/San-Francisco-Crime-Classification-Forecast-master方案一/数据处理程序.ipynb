{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data['Dates'] = pd.to_datetime(data['Dates'])     # 转化为实践序列\n",
    "    \n",
    "    # 提取年、月、日、星期几、小时、时刻    作为新创建的特征\n",
    "    data['year'] = data['Dates'].dt.year\n",
    "    data['month'] = data['Dates'].dt.month\n",
    "    data['day'] = data['Dates'].dt.day\n",
    "    data['wday'] = data['Dates'].dt.dayofweek\n",
    "    data['hour'] = data['Dates'].dt.hour + data['Dates'].dt.minute/60\n",
    "    data['qtr'] = data['Dates'].dt.quarter\n",
    "    \n",
    "    \n",
    "    # one-hot-code\n",
    "    dummy_dayofweef = pd.get_dummies(data['DayOfWeek'],prefix='wday')\n",
    "    data = data.join(dummy_dayofweef)\n",
    "    \n",
    "    # 新增特征，春夏秋冬\n",
    "    def season(x):\n",
    "        spring = 0\n",
    "        summer = 0\n",
    "        fall = 0\n",
    "        winter = 0\n",
    "        if x in [2,3,4]:\n",
    "            spring = 1\n",
    "        if x in [5,6,7]:\n",
    "            summer = 1\n",
    "        if x in [8,9,10]:\n",
    "            fall = 1\n",
    "        if x in [11,12,1]:\n",
    "            winter = 1\n",
    "        return spring,summer,fall,winter\n",
    "    data['spring'],data['summer'],data['fall'],data['winter'] = zip(*data['hour'].apply(season))\n",
    "    \n",
    "    # 新增特征，凌晨、上午、下午、晚上\n",
    "    def time(x):\n",
    "        emorning = 0\n",
    "        morning = 0\n",
    "        afternoon = 0\n",
    "        night = 0\n",
    "        if x>=0 or x<6:\n",
    "            emorning = 1\n",
    "        if x>=6 or x<12:\n",
    "            morning = 1\n",
    "        if x>=12 or x<18:\n",
    "            afternoon = 1\n",
    "        if x>18 or x<=24:\n",
    "            night = 1\n",
    "        return emorning,morning,afternoon,night\n",
    "    data['emorning'],data['morning'],data['afternoon'],data['night'] = zip(*data['hour'].apply(time))\n",
    "    \n",
    "    #删除旧特征\n",
    "    data.drop(['Dates','DayOfWeek','wday'],axis=1,inplace=True)\n",
    "    return data\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_address(train,test):\n",
    "    from copy import deepcopy\n",
    "    # 案发地址集合\n",
    "    addresses = sorted(train['Address'].unique())\n",
    "    # 案发类型集合\n",
    "    categories = sorted(train['Category'].unique())\n",
    "    # 每类案件数量\n",
    "    c_count = train.groupby(['Category']).size()\n",
    "    # 每个案发地址中，每一类案件的发生数量\n",
    "    a_c_count = train.groupby(['Address','Category']).size()\n",
    "    # 每个案发地址的案件数量\n",
    "    a_count = train.groupby(['Address']).size()\n",
    "    # 测试集的案发地址集合\n",
    "    new_addresses = sorted(test['Address'].unique())\n",
    "    # 测试集的每个案发地点的案件数量\n",
    "    new_a_count = test.groupby(['Address']).size()\n",
    "    # 只在测试集中出现的新的案发地址集合\n",
    "    only_new=set(new_addresses+addresses)-set(addresses)\n",
    "    # 同时在测试集和训练集的案发地址集合\n",
    "    in_both = set(new_addresses).intersection(addresses)\n",
    "    \n",
    "    # 存放每个地址，发生每一类案件的概率\n",
    "    logodds={}\n",
    "    # 存放每个地址，发生案件的几率\n",
    "    logoddsPA={}\n",
    "    \n",
    "    # 最小记录数量\n",
    "    min_counts = 2\n",
    "    # 默认地址发生案件的几率\n",
    "    default_logodds = np.log(c_count/len(train)) - np.log(1.0-c_count/float(len(train)))\n",
    "    \n",
    "    \n",
    "    # 迭代每一个地址，对logodds和logoddsPA进行填充\n",
    "    # 训练集案发地址\n",
    "    for addr in addresses:\n",
    "        pa = a_count[addr] / float(len(train))           # addr地址的案件数量 / 所有地址案件数量 = addr地址发生案件的概率\n",
    "        logoddsPA[addr] = np.log(pa) - np.log(1-pa)\n",
    "        logodds[addr] = deepcopy(default_logodds)\n",
    "        for cat in a_c_count[addr].keys():\n",
    "            if (a_c_count[addr][cat]>min_counts and a_c_count[addr][cat]<a_count[addr]):\n",
    "                pa = a_c_count[addr][cat] / float(a_count[addr])                        # addr地址中发生cat案件 / addr地址中所有案件\n",
    "                logodds[addr][categories.index(cat)] = np.log(pa) - np.log(1.0 - pa)     \n",
    "        logodds[addr] = pd.Series(logodds[addr])\n",
    "        logodds[addr].index = range(len(categories))\n",
    "    \n",
    "    # 在测试集中出现的新的案发地址\n",
    "    for addr in only_new:\n",
    "        PA = new_a_count[addr] / float(len(test)+len(train))\n",
    "        logoddsPA[addr] = np.log(PA) - np.log(1.0-PA)\n",
    "        logodds[addr] = deepcopy(default_logodds)\n",
    "        logodds[addr].index = range(len(categories))\n",
    "        \n",
    "    # 训练集、训练集都出现的地址\n",
    "    for addr in in_both:\n",
    "        PA = (a_count[addr]+new_a_count[addr]) / float(len(test)+len(train))\n",
    "        logoddsPA[addr] = np.log(PA) - np.log(1.0-PA)\n",
    "        \n",
    "    # 新增特征：案发地址发生案件的总概率\n",
    "    train['logoddsPA'] = train['Address'].apply(lambda x: logoddsPA[x])\n",
    "    test['loggoddsPA'] = test['Address'].apply(lambda x: logoddsPA[x])\n",
    "    \n",
    "    \n",
    "    # 新增特征: 案发地址发生每一类案件的概率\n",
    "    address_features = train['Address'].apply(lambda x: logodds[x])\n",
    "    address_features.columns = ['logodds' + str(x) for x in range(len(address_features.columns))]\n",
    "    train = train.join(address_features)\n",
    "    address_features = test['Address'].apply(lambda x: logodds[x])\n",
    "    address_features.columns = ['logodds' + str(x) for x in range(len(address_features.columns))]\n",
    "    test = test.join(address_features)\n",
    "    \n",
    "    train.drop('Address',axis=1,inplace=True)\n",
    "    test.drop('Address',axis=1,inplace=True)\n",
    "    return train,test\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ss(train,test):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train)\n",
    "    \n",
    "    train = scaler.transform(train)\n",
    "    test = scaler.transform(test)\n",
    "    return train,test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda1\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype uint8, int32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "D:\\anaconda1\\lib\\site-packages\\ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype uint8, int32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "D:\\anaconda1\\lib\\site-packages\\ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype uint8, int32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    train = pd.read_csv('train.csv')\n",
    "    test = pd.read_csv('test.csv')\n",
    "    test.drop('Id', axis=1, inplace=True)\n",
    "    train.drop(['Descript','Resolution'], axis=1, inplace=True)\n",
    "    \n",
    "    # 经纬度排除\n",
    "    train = train[train['Y'] < 38]\n",
    "    \n",
    "    # 日期处理\n",
    "    train = process_data(train)\n",
    "    test = process_data(test)\n",
    "    \n",
    "    # PdDistrict处理\n",
    "    dummy_pdd_train = pd.get_dummies(train['PdDistrict'], prefix='pdd')\n",
    "    train = train.join(dummy_pdd_train)\n",
    "    dummy_pdd_test = pd.get_dummies(test['PdDistrict'], prefix='pdd')\n",
    "    test = test.join(dummy_pdd_test)\n",
    "    train.drop('PdDistrict', axis=1, inplace=True)\n",
    "    test.drop('PdDistrict', axis=1, inplace=True)\n",
    "    \n",
    "    # 犯罪地点处理\n",
    "    # 判断是否发生在交叉路口\n",
    "    train['interaction'] = train['Address'].apply(lambda x : 1 if '/' in x else 0)\n",
    "    test['interaction'] = test['Address'].apply(lambda x : 1 if '/' in x else 0)\n",
    "    \n",
    "    train,test = process_address(train, test)\n",
    "    \n",
    "    # 目标变量处理\n",
    "    target = train['Category']\n",
    "    num = len(target.unique())       # 犯罪类型的数量\n",
    "    name = list(target.unique())     # 犯罪类型字符串\n",
    "    for i in range(0,num):\n",
    "        target[target.values == name[i]] = i\n",
    "    target_dict = pd.DataFrame(name)\n",
    "    train.drop('Category', axis=1, inplace=True)\n",
    "    \n",
    "    train,test = process_ss(train,test)\n",
    "    \n",
    "    target.to_csv('处理后的数据target.csv', header=False, index=False)\n",
    "    target_dict.to_csv('处理后的数据target_dict.csv', header=False, index=False)\n",
    "    pd.DataFrame(train).to_csv('处理后的数据train.csv', header=False, index=False)\n",
    "    pd.DataFrame(test).to_csv('处理后的数据test.csv', header=False, index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
